{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure a Scenario for FloodAdapt\n",
    "\n",
    "In this notebook we are going to setup a scenario for FloodAdapt to run. This includes a weather event, any adaptation measures and future projections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 0:** Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import tomli_w\n",
    "\n",
    "from DT_flood.utils.workflow_utils import (\n",
    "    create_workflow_config,\n",
    "    run_fa_scenario_workflow,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1:** Set paths to folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the FloodAdapt database for which we are configuring scenarios\n",
    "fa_database = Path(\"/home/wotromp/InterTwin/workflow_refactor/fa_database/Humber\")\n",
    "\n",
    "# Path to data folder. If (relative) file paths are used when specifying data sets, it will be looked for in this folder.\n",
    "# data_folder = Path(\"/home/wotromp/InterTwin/Data\")\n",
    "\n",
    "# OPTIONAL: HydroMT data catalogue from which to fetch data. The order matters here, as datasets are looked for in a catalogue in order from first catalogue name in list to last.\n",
    "# data_catalogues = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2:** Configure the weather Event\n",
    "\n",
    "Here we will specify everything needed to setup an event. For now we will only consider scenarios which involve just a single event.\n",
    "\n",
    "To fully specify an event we will need the following:\n",
    "- A name for referencing\n",
    "- Start and end time and dates\n",
    "- Forcing data for the hazard models. Either paths to data files or keys for data catalogues. See HydroMT (SFINCS,WFlow) for naming conventions of data variables\n",
    "    - When using a single dataset for meteorological data this needs to contain precipitation, wind speeds, and pressure. Otherwise use individual data sources for each\n",
    "\n",
    "WFlow requires two sets of forcings, one for the warmup run, and one for the event run. See HydroMT WFlow `setup_precip_forcing` and `setup_temp_pet_forcing` which data variables should be included (conveniently ERA5 provides them all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of event\n",
    "event_name = 'test_event_rucio'\n",
    "\n",
    "# Start and end times of the event\n",
    "start_time = '2013-12-04 00:00:00'\n",
    "end_time = '2013-12-10 00:00:00'\n",
    "\n",
    "# Forcing data for the sfincs model.\n",
    "sfincs_meteo = 'era5_hourly'\n",
    "sfincs_waterlevel = 'gtsm_hourly'\n",
    "\n",
    "wflow_precip_warmup = \"era5_daily\"\n",
    "wflow_pet_warmup = \"era5_daily\"\n",
    "wflow_precip_event = \"era5_hourly\"\n",
    "wflow_pet_event = \"era5_hourly\"\n",
    "wflow_orography = \"era5_orography\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {\n",
    "    'name': event_name,\n",
    "    'start_time': start_time,\n",
    "    'end_time': end_time,\n",
    "    # \"data_catalogues\":  [str(path) for path in data_catalogues],\n",
    "    'sfincs_forcing': {\n",
    "        'meteo': sfincs_meteo,\n",
    "        'waterlevel': sfincs_waterlevel\n",
    "    },\n",
    "    # \"sfincs_forcing_offshore\": sfincs_forcing_offshore\n",
    "    \"wflow_forcing\": {\n",
    "        'precip_warmup': wflow_precip_warmup,\n",
    "        'pet_warmup': wflow_pet_warmup,\n",
    "        'precip_event': wflow_precip_event,\n",
    "        'pet_event': wflow_pet_event,\n",
    "        \"orography\": wflow_orography\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3:** Configure projections\n",
    "\n",
    "Here we specify, where applicable, the projected future changes relevant to our scenario. These come in two flavors: physical projections and socio-economic changes. The former contains changes on the hazard side (sea level rise, land subsidence etc.) while the latter contains changes to the impact side (population growth, economic growth etc.).\\\n",
    "The supported projections are:\\\n",
    "\\\n",
    "**Physical Projections:**\n",
    "- Sea level rise\n",
    "- Land subsidence\n",
    "- Rainfall increase\n",
    "- Storm frequency increase\n",
    "\n",
    "**Socio-Economic Projections:**\n",
    "- Population growth\n",
    "- Economic growth\n",
    "- New developments:\n",
    "    - Newly developed area\n",
    "    - Elevation of newly developed area\n",
    "    - Population growth in newly developed area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of projection\n",
    "projection_name = 'empty_projection'\n",
    "\n",
    "# Physical projections\n",
    "sea_level_rise = 0\n",
    "rainfall_increase = 0\n",
    "\n",
    "\n",
    "# Socio-Economic projections\n",
    "pop_growth = 0\n",
    "economic_growth = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dict = {\n",
    "    'name': projection_name,\n",
    "    'physical_projection': {\n",
    "        'sea_level_rise': sea_level_rise,\n",
    "        'rainfall_increase': rainfall_increase\n",
    "    },\n",
    "    'socio_economic_change': {\n",
    "        'population_growth_existing': pop_growth,\n",
    "        'economic_growth': economic_growth\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4:** Configure Measures\n",
    "\n",
    "Here we configure all the measure we want to implement for this scenario. These come in either Impact or Hazard types, either reducing the vulnerability to a hazard, or dampen the hazard itself. In either case we need to specify in which area the measure acts by specifying a geojson file or in the case of Impact types an area name recognized by the aggregation area file. For the Impact type we also need to specify which building type to act on.\\\n",
    "The supported measures are:\\\n",
    "\\\n",
    "**Hazard Type:**\n",
    "- Floodwall\n",
    "- Pump\n",
    "- Water square\n",
    "- Green infrastructure\n",
    "- Water storage\n",
    "\n",
    "**Impact Type:**\n",
    "- Elevate properties\n",
    "- Floodproof properties\n",
    "- Buyout properties\n",
    "\n",
    "All the measures needed for the scenario are collected into a single strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to reset the measures to include in the scenario\n",
    "strategy_name = 'empty_strategy'\n",
    "strategy = {'name': strategy_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun this cell multiple times to include multiple measures, everytime with updated variables appropriate to the to-be-added measure\n",
    "\n",
    "# Name of measure\n",
    "measure_name = 'empty_measure'\n",
    "\n",
    "# Measure type\n",
    "measure_type = 'floodwall'\n",
    "measure_area = str(fa_database / 'HumberDelta_weir.geojson')\n",
    "\n",
    "# These settings depend on the measure, already in a FloodAdapt format\n",
    "measure_misc = {\"elevation\": {\"value\": 2, \"units\": \"meters\"}}\n",
    "\n",
    "measure_dict = {\n",
    "    'name': measure_name,\n",
    "    # 'type': measure_type,\n",
    "    # 'selection': measure_area,\n",
    "    # 'misc': measure_misc,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy.update({f\"measure{len(strategy)}\": measure_dict})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5:** Collect everything into a single Scenario\n",
    "\n",
    "Now that we have configured each individual element of a scenario (event, projection, measure) we can collect everything together into a single scenario definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = {\n",
    "    'name': \"test_rucio_scenario\",\n",
    "    'event': event_dict,\n",
    "    'projection': proj_dict,\n",
    "    'strategy': strategy,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to toml file. The _toplevel suffix indicated that this is a toml file also configuring non-FloodAdapt functionalities\n",
    "scenario_fn = fa_database / \"input\" / \"scenarios\" / scenario[\"name\"] / f\"{scenario['name']}_toplevel.toml\"\n",
    "\n",
    "if not scenario_fn.parent.exists():\n",
    "    scenario_fn.parent.mkdir(parents=True)\n",
    "\n",
    "with open(scenario_fn, 'wb+') as f:\n",
    "    tomli_w.dump(scenario, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 6:** Launch workflow\n",
    "\n",
    "With a complete definition of the scenario we can run the workflow. First we export the cwl config file based on the scenario settings we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import tomli_w\n",
    "from DT_flood.utils.workflow_utils import create_workflow_config, run_fa_scenario_workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_fn = Path(\"/home/wotromp/InterTwin/workflow_refactor/fa_database/Humber\")\n",
    "scenario_name = \"test_rucio_scenario\"\n",
    "\n",
    "oscar_endpoint = 'https://oscar-ukri.intertwin.fedcloud.eu'\n",
    "oscar_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rucio.client\n",
    "\n",
    "\n",
    "client = rucio.client.Client()\n",
    "client.list_accounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_workflow_config(database_fn, scenario_name, oscar_endpoint=oscar_endpoint, oscar_token=oscar_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fa_scenario_workflow(database_fn, scenario_name, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DT_flood.utils.fa_scenario_utils import init_scenario\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db, scenario = init_scenario(database_path=database_fn, scenario_config_name=scenario_name+\"_toplevel.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(vars(db.site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiat_path = db.output_path/\"scenarios\"/\"test_scenario\"/\"Impacts\"/\"fiat_model\"\n",
    "hazard_fn, = glob.glob(str(fiat_path/\"hazard\")+\"/*.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"risk\" in Path(hazard_fn).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(hazard_fn).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test github acccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"/home/wotromp/InterTwin/workflow_refactor/fa_database/Humber/input/events/test_event_rucio/meteo.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"latitude\" in ds.coords:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.to_datetime(\"2013-12-04 00:00:00\")-pd.DateOffset(years=1)).strftime(\"%Y:%m:%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DT-flood",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
